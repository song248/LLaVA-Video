import os
import torch
import numpy as np
from transformers import AutoTokenizer, AutoModelForCausalLM, AutoImageProcessor
from peft import PeftModel, PeftConfig
from decord import VideoReader, cpu
from PIL import Image
import torchvision.transforms as T

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì„¤ì •
video_path = "infer.mp4"
adapter_path = "./work_dirs/llava-lora-fight-vqa-google_siglip-so400m-patch14-384-Qwen_Qwen1.5-1.8B"
base_model = "Qwen/Qwen1.5-1.8B"
vision_tower = "google/siglip-so400m-patch14-384"
device = "cuda" if torch.cuda.is_available() else "cpu"
question = ('Just answer "Yes" or "No". Did this video show any physical violence between people?')
# question = (
#     "Did this video contain any physical altercations or violent incidents between people, "
#     "such as punching, pushing, kicking, threatening gestures, or any other aggressive or hostile behavior?"
# )

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë¹„ë””ì˜¤ â†’ í”„ë ˆì„ ì¶”ì¶œ
def extract_video_frames(path, num_frames=8):
    vr = VideoReader(path, ctx=cpu(0))
    total = len(vr)
    idxs = np.linspace(0, total - 1, num_frames, dtype=int)
    frames = vr.get_batch(idxs).asnumpy()  # (T, H, W, C)
    pil_frames = [Image.fromarray(f) for f in frames]
    return pil_frames

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ëª¨ë¸ ë¡œë“œ
print("Loading tokenizer and base model...")
tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    base_model,
    device_map="auto",
    trust_remote_code=True
)

# LoRA ì—°ê²° (ë¡œì»¬ ê²½ë¡œ ì£¼ì˜!)
print(f"Loading LoRA adapter from local path: {adapter_path}")
peft_config = PeftConfig.from_pretrained(adapter_path, local_files_only=True)
model = PeftModel.from_pretrained(
    model,
    adapter_path,
    is_trainable=False,
    local_files_only=True
).to(device).eval()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œ
processor = AutoImageProcessor.from_pretrained(vision_tower)
image_transform = T.Compose([
    T.Resize((384, 384)),  # SigLIP patch14-384
    T.ToTensor(),
    T.Normalize(mean=processor.image_mean, std=processor.image_std)
])

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í”„ë ˆì„ ì „ì²˜ë¦¬ ë° í…ìŠ¤íŠ¸ ì—°ê²°
frames = extract_video_frames(video_path, num_frames=8)
frame_tensor = torch.stack([image_transform(f) for f in frames])  # (T, C, H, W)
frame_tensor = frame_tensor.unsqueeze(0).to(device)  # (1, T, C, H, W)

# í…ìŠ¤íŠ¸ prompt êµ¬ì„±
prompt = f"[Video with {len(frames)} frames]\n{question}\nAnswer:"
inputs = tokenizer(prompt, return_tensors="pt").to(device)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í…ìŠ¤íŠ¸ ìƒì„±
print("Running inference...")
with torch.no_grad():
    output_ids = model.generate(
        input_ids=inputs.input_ids,
        attention_mask=inputs.attention_mask,
        max_new_tokens=50,
        do_sample=False
    )

response = tokenizer.decode(output_ids[0], skip_special_tokens=True)
print("\nğŸ“½ï¸ ì§ˆë¬¸:", question)
print("ğŸ§  ë‹µë³€:", response)
